{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b5bf07",
   "metadata": {},
   "source": [
    "# TV-Shows data analytics\n",
    "#### This notebook explores a dataset of TV-Shows, using pyspark. The information gained could be of value to a market-researcher, content-creator or TV enthusiast. The data is imported in JSON format, as a Spark Dataframe. Some visualisations have been created using Seaborn by converting the data to a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373aea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5559b",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTVShowAnalysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      2\u001b[39m data = spark.read.json(file_path)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n",
      "\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n",
      "\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n",
      "\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n",
      "\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n",
      "\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n",
      "\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m    199\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n",
      "\u001b[32m    204\u001b[39m         master,\n",
      "\u001b[32m    205\u001b[39m         appName,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n",
      "\u001b[32m    216\u001b[39m     )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n",
      "\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n",
      "\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n",
      "\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n",
      "\u001b[32m    104\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n",
      "\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n",
      "\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n",
      "\u001b[32m    108\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m    109\u001b[39m         message_parameters={},\n",
      "\u001b[32m    110\u001b[39m     )\n",
      "\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n",
      "\u001b[32m    113\u001b[39m     gateway_port = read_int(info)\n",
      "\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"TVShowAnalysis\").getOrCreate()\n",
    "data = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a5ba6d",
   "metadata": {},
   "source": [
    "#### Look at a data row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8d9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45f8e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8374f149",
   "metadata": {},
   "source": [
    "#### Return number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7daa9",
   "metadata": {},
   "source": [
    "#### Remove duplicate rows, and return row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e020b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropDuplicates()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de94769",
   "metadata": {},
   "source": [
    "#### View column datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3176f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b3702",
   "metadata": {},
   "source": [
    "#### View schema info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6967edae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1076f95",
   "metadata": {},
   "source": [
    "#### View a selection of title names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a91419",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.select(\"name\").limit(10))\n",
    "display(data.select(\"name\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94dcbd1",
   "metadata": {},
   "source": [
    "#### Count number of shows in each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02905860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where 'language' column is not null\n",
    "filtered_data = data.filter(col('language').isNotNull())\n",
    "display(filtered_data.groupBy('language').count().count())\n",
    "# Group by 'language' column and count occurrences, then sort in descending order and limit to 7 rows\n",
    "sorted_data = filtered_data.groupBy('language').count().orderBy(desc('count')).limit(7)\n",
    "\n",
    "# Display the sorted and limited data\n",
    "display(sorted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184173ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"tv_shows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_data = spark.sql(\"\"\"\n",
    "          SELECT DISTINCT genres\n",
    "          FROM tv_shows\n",
    "         \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83011513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `genres_data` is your DataFrame containing the column of arrays\n",
    "distinct_genres = genres_data.select(explode(\"genres\").alias(\"genre\")).distinct()\n",
    "\n",
    "# Show the distinct genres\n",
    "distinct_genres.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d2dc4",
   "metadata": {},
   "source": [
    "#### View count by show-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc46061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_data = data.groupby('type').count()\n",
    "display(type_data)\n",
    "types_pandas = type_data.toPandas()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(data=types_pandas, x='count', y='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d8b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data.groupby('averageRuntime').count()\n",
    "display(x_data.limit(5))\n",
    "\n",
    "pandas_df = x_data.toPandas()\n",
    "sns.displot(data=pandas_df, kind=\"kde\", x='averageRuntime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f302be2",
   "metadata": {},
   "source": [
    "#### To conclude, some aspects of this large dataset have been revealed by using Pyspark. For instance, it was shown that scripted shows are by far the most common in the set, followed by documentaries, reality shows and animation. A total of 73 languages were found, the most prevalent being English, then Japanese, Russian and Korean. The distribution curve investigating average-runtime peaks at just under 100 minutes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
