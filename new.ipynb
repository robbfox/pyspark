{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30cc7e7-d073-4cd4-9eec-151f4d9bd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialization & Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c4e836-ace5-4a58-b8bc-392c5d2aa146",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# import json # Not used in the provided snippet\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTVShowAnalysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Define file_path BEFORE using it\u001b[39;00m\n\u001b[32m     12\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m/FileStore/tables/tv_shows.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    108\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m         message_parameters={},\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    113\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "# Imports should ideally be grouped at the top\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, desc, explode # Added required functions here\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt # Keep even if not directly used, seaborn uses it\n",
    "import pandas as pd\n",
    "# import json # Not used in the provided snippet\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TVShowAnalysis\").getOrCreate()\n",
    "\n",
    "# Define file_path BEFORE using it\n",
    "file_path = \"/FileStore/tables/tv_shows.json\"\n",
    "\n",
    "# Use spark.read, not sqlContext.read (modern API)\n",
    "# Remove the first attempt to read before file_path was defined\n",
    "data = spark.read.json(file_path)\n",
    "\n",
    "# Look at a data row (optional, good for check)\n",
    "# data.show(1, truncate=False, vertical=True) # Alternative way to view a row nicely\n",
    "print(data.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5a5d4-8cba-499a-9f8e-fb6f3b0bd94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Row Counts & Duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605369c7-fbba-428b-8d14-3742dfbef6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return number of rows\n",
    "initial_count = data.count()\n",
    "print(f\"Initial row count: {initial_count}\")\n",
    "\n",
    "# Remove duplicate rows, and return row count\n",
    "# dropDuplicates() returns a NEW DataFrame, it doesn't modify 'data' in-place\n",
    "data_deduplicated = data.dropDuplicates()\n",
    "deduplicated_count = data_deduplicated.count()\n",
    "print(f\"Row count after dropDuplicates: {deduplicated_count}\")\n",
    "\n",
    "# If you intend to use the deduplicated data from now on:\n",
    "# data = data_deduplicated\n",
    "# Otherwise, subsequent operations use the original 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca18ea-8c8c-44cc-bf40-7ed01cae7600",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Schema Inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02818d-9830-48f1-b487-6b19c8a38949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View column datatypes (returns list of tuples)\n",
    "print(data.dtypes)\n",
    "\n",
    "# View schema info (prints tree structure)\n",
    "# printSchema is a method, needs parentheses ()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f1d61-ba8d-4caf-a7db-1c98178d521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploring Names & Languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655c473-4b97-4244-8841-d397478fc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a selection of title names\n",
    "display(data.select(\"name\").limit(10))\n",
    "\n",
    "# Display total count of rows (or non-null names)\n",
    "# data.select(\"name\").count() is the same as data.count() unless names can be null\n",
    "# Maybe you wanted distinct names? data.select(\"name\").distinct().count()\n",
    "print(f\"Total shows (data.count()): {data.count()}\")\n",
    "\n",
    "# Count number of shows in each language\n",
    "# Filter out rows where 'language' column is not null\n",
    "filtered_data = data.filter(col('language').isNotNull())\n",
    "\n",
    "# Count distinct languages\n",
    "distinct_language_count = filtered_data.select('language').distinct().count()\n",
    "print(f\"Total distinct languages found: {distinct_language_count}\") # Matches the 73 mentioned in summary\n",
    "\n",
    "# Group by 'language' column and count occurrences, get top 7\n",
    "# You already filtered nulls, so you can apply groupBy directly to filtered_data\n",
    "language_counts = filtered_data.groupBy('language').count().orderBy(desc('count'))\n",
    "\n",
    "# Display the top 7\n",
    "display(language_counts.limit(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdfccb1-aa74-46c0-aa3e-9b330fa9a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genre Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b5427-e072-4993-a2de-d9d248d7cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode genres directly from the main DataFrame\n",
    "# Assuming 'genres' is an array column in the 'data' DataFrame\n",
    "# No need for the temporary view and SQL unless preferred\n",
    "\n",
    "# Check if 'genres' column exists and is of array type first (optional but good practice)\n",
    "if 'genres' in dict(data.dtypes) and data.schema['genres'].dataType.typeName() == 'array':\n",
    "    distinct_genres = data.select(explode(\"genres\").alias(\"genre\")).distinct()\n",
    "    print(\"Distinct Genres:\")\n",
    "    display(distinct_genres) # Use display for better formatting in Databricks\n",
    "else:\n",
    "    print(\"Column 'genres' not found or is not an ArrayType.\")\n",
    "\n",
    "# --- Alternative using SQL (if preferred) ---\n",
    "# data.createOrReplaceTempView(\"tv_shows_view\")\n",
    "# distinct_genres_sql = spark.sql(\"\"\"\n",
    "#     SELECT DISTINCT exploded_genre\n",
    "#     FROM tv_shows_view\n",
    "#     LATERAL VIEW explode(genres) exploded_table AS exploded_genre\n",
    "# \"\"\")\n",
    "# print(\"Distinct Genres (SQL method):\")\n",
    "# display(distinct_genres_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910dcd8-e5d5-402b-ad3e-b414452ecaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show Type Analysis & Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204630bb-28d2-4b17-8b14-f065aa62a7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View count by show-type\n",
    "type_data = data.groupby('type').count().orderBy(desc('count')) # Added order for consistency\n",
    "print(\"Counts by Show Type:\")\n",
    "display(type_data)\n",
    "\n",
    "# Convert to Pandas for plotting (fine for small aggregated data)\n",
    "types_pandas = type_data.toPandas()\n",
    "\n",
    "# Plotting\n",
    "sns.set_theme(style=\"whitegrid\") # Use set_theme for modern seaborn\n",
    "plt.figure(figsize=(8, 4)) # Optional: Adjust figure size\n",
    "sns.barplot(data=types_pandas, x='count', y='type')\n",
    "plt.title('Number of Shows by Type') # Add a title\n",
    "plt.show() # Explicitly show plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa440b-db6d-476a-89ee-5b8b6ca46c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Runtime Analysis & Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b3f8e-404a-494a-a455-b7b5a52a41de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Average Runtime distribution\n",
    "# Grouping by runtime and counting is valid but doesn't directly give the distribution plot desired\n",
    "# x_data = data.groupby('averageRuntime').count()\n",
    "# display(x_data.limit(5))\n",
    "\n",
    "# For plotting the distribution of runtimes, work with the column directly\n",
    "# Filter out potential nulls or invalid values if necessary\n",
    "runtime_data_pd = data.select(\"averageRuntime\").filter(col(\"averageRuntime\").isNotNull()).toPandas()\n",
    "\n",
    "# Plot the distribution (KDE)\n",
    "sns.displot(data=runtime_data_pd, x='averageRuntime', kind=\"kde\")\n",
    "plt.title('Distribution of Average Show Runtime (minutes)')\n",
    "plt.show()\n",
    "\n",
    "# -- If the dataset is HUGE, sample before toPandas --\n",
    "# sample_fraction = 0.1 # Adjust as needed\n",
    "# runtime_data_pd_sampled = data.select(\"averageRuntime\") \\\n",
    "#                                .filter(col(\"averageRuntime\").isNotNull()) \\\n",
    "#                                .sample(withReplacement=False, fraction=sample_fraction) \\\n",
    "#                                .toPandas()\n",
    "# sns.displot(data=runtime_data_pd_sampled, x='averageRuntime', kind=\"kde\")\n",
    "# plt.title(f'Distribution of Average Show Runtime (Sampled {int(sample_fraction*100)}%)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778458b-ea5c-4408-ad96-3417e4e6865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
